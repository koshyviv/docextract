# -*- coding: utf-8 -*-
"""Intro to LOTUS.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1qtmklXD_J1SSJLR86ws4GsYcLln6FZqY

# Getting Started with LOTUS
<!--- BADGES: START --->
[![LOTUS](https://img.shields.io/static/v1?label=repo&message=lotus&color=purple&logo=github)](https://github.com/TAG-Research/lotus)
[![Arxiv](https://img.shields.io/badge/arXiv-2407.11418-B31B1B.svg)][#arxiv-paper-package]
[![Documentation Status](https://readthedocs.org/projects/lotus-ai/badge/?version=latest)](https://lotus-ai.readthedocs.io/en/latest/?badge=latest)
[![PyPI - Python Version](https://img.shields.io/pypi/pyversions/lotus-ai)][#pypi-package]
[![PyPI](https://img.shields.io/pypi/v/lotus-ai)][#pypi-package]
[![GitHub license](https://img.shields.io/badge/License-MIT-blu.svg)][#license-gh-package]



[#license-gh-package]: https://lbesson.mit-license.org/
[#arxiv-paper-package]: https://arxiv.org/abs/2407.11418
[#pypi-package]: https://pypi.org/project/lotus-ai/
<!--- BADGES: END --->

This demo introduces **LOTUS**, an LLM-powered query engine for processing unstructured and structured data.

LOTUS implements **semantic operators**, a powerful, **declarative programming model** for representing semantic transformations, parameterized by natural language expressions, over datasets. Semantic operators can be implemented and optimized with multiple AI-based algorithms, opening a rich space for execution plans, similar to relational operators.

LOTUS implements a core set of semantic operators in an intuitive Pandas-like API. LOTUS also **transparently optimizes** user programs to provide highly **efficient query execution**.


Below we provide an overview of core semantic operators supported by LOTUS. We will then demonstrate how to compose semantic operators in a few lines of code to build a powerful AI-based pipeline for searching and processing ArXiv papers.

## 0. Installation

Run the following cells to install required dependencies.
"""

# Commented out IPython magic to ensure Python compatibility.
# %pip install arxiv lotus-ai gradio pymupdf
# %pip install git+https://github.com/lotus-data/lotus.git@main

# !git clone https://github.com/lotus-db/Arxiv-Papers-Demo.git data

"""## 1. Setup
Let's set up LOTUS and load in the papers dataset we will use.
"""

# Commented out IPython magic to ensure Python compatibility.
# %set_env OPENAI_API_KEY=

"""### Configure Models in LOTUS

First we will set up the LOTUS system with the desired models and parameters. We will use gpt-4o-mini as our underlying language model and E5 as the embedding model. LOTUS also allows users to configure additional parameters such as maximum tokens and batch size.
"""

import lotus
import arxiv
import pandas as pd
import gradio as gr

import json, re, time, os
from datetime import datetime, timedelta
from html import unescape
from requests import Session, get
from tqdm import tqdm
from lotus.vector_store import FaissVS

OPENAI_API_KEY=os.environ["OPENAI_API_KEY"]

if len(OPENAI_API_KEY) == 0:
    raise Exception("OpenAI API Key is required")

lotus.settings.configure(
    lm=lotus.models.LM(
        api_key=OPENAI_API_KEY,
        model="gpt-4.1-nano",
    ),
    rm = lotus.models.LiteLLMRM(model="text-embedding-3-small"),
    vs = FaissVS()
)

# NOTE: we often notice a collab issue upon running this cell. To fix, you can restart the runtime and rerun; and the error should go away

"""### Loading the Papers Dataset

Now that we have configured LOTUS, let's load in our papers dataset that we will use to display LOTUS in action. The papers dataframe contains information about recent arxiv papers such as title, authors, abstract, when it was published, and the maximum h-index across all the authors. The papers are mostly related to machine learning and databases.
"""

papers_df = pd.read_csv("data/all_papers_df.csv")
sampled_papers_df = papers_df.head(20)
sampled_papers_df.head()

"""## 2. Semantic Operators Introduction

Semantic operators are declarative NL-based transformations on one or more datasets, and can be implemented by multiple AI-based algorithms. Each semantic operator is specified a parameterized natural language
expressions (langex for short), which are natural language expressions that specify a function over one or more attributes for the
given semantic transformation. The langex signature depends on its
semantic operator and may represent a predicate, aggregation, comparator function, or projection in natural language. Below is a table of the semantic operators available in LOTUS, but it is possible to implement other semantic operators.

<table style="width:100%; border: 1px solid black; border-collapse: collapse;">
  <caption style="caption-side: top; font-size: 1.5em; font-weight: bold; padding: 10px;">
    2. Semantic Operators Introduction
  </caption>
  <thead>
    <tr style="background-color: #f2f2f2;">
      <th style="border: 1px solid black; padding: 8px; text-align: left;">Operator</th>
      <th style="border: 1px solid black; padding: 8px; text-align: left;">Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="border: 1px solid black; padding: 8px;">sem_map</td>
      <td style="border: 1px solid black; padding: 8px;">Map each record using a natural language projection</td>
    </tr>
    <tr>
      <td style="border: 1px solid black; padding: 8px;">sem_filter</td>
      <td style="border: 1px solid black; padding: 8px;">Keep records that match the natural language predicate</td>
    </tr>
    <tr>
      <td style="border: 1px solid black; padding: 8px;">sem_agg</td>
      <td style="border: 1px solid black; padding: 8px;">Performs a natural language aggregation across all records (e.g., for summarization)</td>
    </tr>
    <tr>
      <td style="border: 1px solid black; padding: 8px;">sem_topk</td>
      <td style="border: 1px solid black; padding: 8px;">Order the records by some natural language sorting criteria</td>
    </tr>
    <tr>
      <td style="border: 1px solid black; padding: 8px;">sem_join</td>
      <td style="border: 1px solid black; padding: 8px;">Join two datasets based on a natural language predicate</td>
    </tr>
    <tr>
      <td style="border: 1px solid black; padding: 8px;">sem_dedup</td>
      <td style="border: 1px solid black; padding: 8px;">Deduplicate records based on semantic similarity</td>
    </tr>
    <tr>
      <td style="border: 1px solid black; padding: 8px;">sem_index</td>
      <td style="border: 1px solid black; padding: 8px;">Create a semantic similarity index over a text column</td>
    </tr>
    <tr>
      <td style="border: 1px solid black; padding: 8px;">sem_search</td>
      <td style="border: 1px solid black; padding: 8px;">Perform top-k search over a text column</td>
    </tr>
  </tbody>
</table>

### Semantic Filter

Semantic filter returns the subset of rows that pass the filtering condition, specified by the user's langex. In the example below, the langex specifies the title attribute and is used to only keep titles are related to recommendation systems.
"""

sampled_papers_df.sem_filter("{title} is related to recommendation systems")

"""### Semantic TopK

Semantic top K ranks a set of rows according to the langex, and returns the K rows that best match the ranking criteria. The signature of the langex provides a general ranking criteria according to one or more columns. Let's see how it can be used to find the paper with the best acronyms.
"""

sampled_papers_df.sem_topk("{abstract} has the best acronym", K=1).iloc[0].abstract

"""### Semantic Join

Semantic join combines data from two tables, evaluating the user's langex to return the set of rows from the left and right table that pass. Let's see how it can be used to match abstracts with topics.
"""

# let's create a new dataframe to perform a join with our papers
topics_df = pd.DataFrame({
    "topic": ["vector database", "probability"]
})
topics_df

joined_df = sampled_papers_df.sem_join(topics_df, "{abstract} is about {topic}")
joined_df[["title", "abstract", "topic"]]

"""### Data Types

As we showed above, LOTUS‚Äô data model consists of tables with structured and unstructured text fields. LOTUS‚Äô semantic operators can
take both of these data-types as inputs.
Additionally, LOTUS supports semantic indices over natural language text columns to provide optimized query processing.
These indices leverage semantic embeddings over each document
in the column to capture semantic similarity using embedding
distance metrics. Let's see how we can build and search an index to find titles related to language models.
"""

sampled_papers_df = sampled_papers_df.sem_index("title", "title_index_dir")
sampled_papers_df.sem_search("title", "language models", K=3)[["title"]]

"""## 3. Composing Semantic Operators to Build Powerful AI Apps

Now we'll show how we can compose semantic operators with LOTUS into powerful reasoning-based pipelines, in only a few lines of code!

In this section, we'll build an application that allows us to process recent ArXiv papers based on:
- a user's research interests (e.g. retrieval augmented generation)
- relevance filtering criteria (eg. "the paper claim to outperform GPT-3")
- the h-index of authors on the paper
- the paper publication date
- the arxiv domain of the paper

Our app will both (a) return relevant papers according to the search entry, and
(b) summarize the relevant papers

The final app will look something like this:

### Using Relational Operators in LOTUS

LOTUS supports executing relational operators through a Pandas-like API. Let's first filter our papers dataframe to only contain cs.AI/ML papers posted after 10/5/24 with a max author h-index above 2.
"""

HINDEX_THRESHOLD = 2
SELECTED_DOMAINS = ["cs.AI", "cs.ML"]
PAPERS_AFTER = (2024, 10, 5)

# filter by author hindex
filtered_papers = papers_df[papers_df["max_author_hindex"] >= HINDEX_THRESHOLD]

# filter by categories
filtered_papers = filtered_papers[
    filtered_papers["categories"].apply(lambda x: any([d in x for d in SELECTED_DOMAINS]))
    ]

filtered_papers["date_published"] = pd.to_datetime(filtered_papers["date_published"], errors='coerce')
# filter by date
filtered_papers = filtered_papers[
     filtered_papers["date_published"].dt.date >= datetime(*PAPERS_AFTER).date()
    ]
filtered_papers

"""### Using Semantic Operators in LOTUS
We can use LOTUS to make semantic queries over the dataset. Let's say we want to select papers that:
1. Are related to some research topics
2. Fit into some relevance criteria
"""

# search by research topics
RESEARCH_TOPICS = "retrieval augmented generation"
filtered_papers = filtered_papers.load_sem_index("abstract", "data/abstract_index_dir")
filtered_papers = filtered_papers.sem_search("abstract", RESEARCH_TOPICS, K=10)
filtered_papers

RELEVANCE_CRITERIA = "provides an open-source repository of their code"
filtered_papers = filtered_papers.sem_filter(
  f"Based on the paper {{abstract}}, the paper meets the following criteria: {RELEVANCE_CRITERIA}"
)
filtered_papers

filtered_papers.iloc[0].abstract

"""LOTUS can also summarize the filtered papers for you"""

# First generate a draft summary
summary_df = filtered_papers.sem_agg(
  "You are writing a digest for a user who wants to catch up on recent papers. "
  +"given each paper {abstract}, connect the unifying themes among papers. "
  +f"Also make sure to draw connections between the papers and the users interests, which are: {RESEARCH_TOPICS}",
  suffix="draft_summary",
)

# Refine the summary
summary_df = summary_df.sem_map(
  "Given the {draft_summary}, write a refined summary of the main topics discussed in the "
  +"papers and how they relate to the user's interests. Be sure to provide "
  +"connect the unifying themes among the papers. Also make sure to draw "
  +"connections between the papers and the users interests, which are: "
  + f"{RESEARCH_TOPICS}",
  suffix="final_summary",
)

summary_df.iloc[0].final_summary

"""### Final App
Let's bring this all together in a single gradio app. The app will use the previously downloaded dataset and user input to return the required list of papers and their summary.

The `greet` function contains the core logic of our app.
"""

def greet(
    research_topics = "",
    relevance_criteria = "",
    domain_filter = [],
    start_date = datetime(2024,9,5),
    hindex_threshold = 0,
):
  filtered_papers = papers_df.copy().load_sem_index("abstract", "data/abstract_index_dir")
  filtered_papers["date_published"] = pd.to_datetime(filtered_papers["date_published"], errors='coerce')
  if hindex_threshold > 0:
    filtered_papers = filtered_papers[filtered_papers["max_author_hindex"] >= hindex_threshold]
  if len(domain_filter) > 0:
    filtered_papers = filtered_papers[
      filtered_papers["categories"].apply(lambda x: any([d in x for d in domain_filter]))
    ]
  if isinstance(start_date,datetime):
    filtered_papers = filtered_papers[
      filtered_papers["date_published"].dt.date >= start_date.date()
    ]
  if len(filtered_papers) > 0 and research_topics != "":
    filtered_papers = filtered_papers.sem_search("abstract", research_topics, K=20)
  if len(filtered_papers) > 0 and relevance_criteria != "":
    filtered_papers = filtered_papers.sem_filter(
      "Based on the paper {title} and {abstract}, the paper meets the following criteria: "
      + f"{relevance_criteria}"
    )

  summary = f"Found {len(filtered_papers)} papers:"
  if len(filtered_papers) > 0 and research_topics != "":
    summary = filtered_papers.sem_agg(
      "You are writing a digest for a user who wants to catch up on recent papers. Write a summary discussing the main topics and problems discussed in the papers, given each paper {abstract} and {link}, which you should use to provide citations. Connect the unifying themes among papers. Be sure to equally discuss all provided papers. Also make sure to draw connections between the papers and the users interests, which are: "
      + f"{research_topics}",
      suffix="papers",
    ).sem_map(
      "Given the {papers}, write a summary of the main topics discussed in the papers and how they relate to the user's interests. Be sure to provide connect the unifying themes among the papers. Also make sure to draw connections between the papers and the users interests, which are: "
      + f"{research_topics}",
      suffix="final_summary",
    ).iloc[0].final_summary


  filtered_papers["title (click on me!)"] = filtered_papers.apply(
    lambda x: f"[{x['title']}]({x['link']})", axis=1
  )
  return summary, filtered_papers[["title (click on me!)", "abstract"]]

"""Let's now add the UI for this app"""

with gr.Blocks() as demo:
    with gr.Tab("ArXiv Paper Finder"):
        gr.Markdown(
            "üîç Enter your search criteria below and then click **Run** to find your relevant ArXiv papers."
        )
        # gr.Markdown(
        #     "üìù Please share your anonymous feedback [here](https://docs.google.com/forms/d/e/1FAIpQLScPC8rYFCEBoFiXuMHoN31vIGgPrp8ix6vEEaJt3zpcM2xdtg/viewform?usp=sf_link) üôè"
        # )
        with gr.Row():
                with gr.Column():
                    summary = gr.Markdown("")
                    papers = gr.Dataframe(
                        headers=["Title", "Abstract"],
                        datatype=["markdown", "str"],
                        row_count=5,
                        col_count=(2, "fixed"),
                        column_widths=["30%", "70%"],
                        wrap=True,
                        interactive=True,
                    )
        with gr.Row():
            gr.Markdown("üëâ Enter one or more filtering criteria.\n")
            research_topics = gr.Textbox(
                placeholder="What are your research interests?",
                value="retrieval augmented generation",
                label="Research Interests",
            )
            relevance_criteria = gr.Textbox(
                placeholder="Enter additional relevance critiera",
                value="provides an open-source repository of their code",
                label="Relevance Criteria",
            )
            hindex_threshold = gr.Slider(
                value=2, minimum=0, maximum=100, step=1, label="Author H-index threshold"
            )
            start_date = gr.DateTime(
                value=datetime(2024,10,5).strftime("%Y-%m-%d"),
                include_time=False,
                type='datetime',
                label="Only show papers after",
            )
            domain_filter = gr.CheckboxGroup(
                ["cs.AI", "cs.DB", "cs.IR", "cs.PL", "cs.ML", "cs.CL"],
                label="Arxiv Domains",
                value=["cs.AI"],
            )
        btn = gr.Button("Run")
        btn.click(
            fn=greet,
            inputs=[research_topics, relevance_criteria, domain_filter, start_date, hindex_threshold],
            outputs=[summary, papers],
        )
    with gr.Tab("Browse the Full Arxiv Corpus"):
        all_papers = papers_df.copy()
        all_papers["title (click on me!)"] = all_papers.apply(
            lambda x: f"[{x['title']}]({x['link']})", axis=1
        )
        gr.Markdown(f"## Corpus of {len(all_papers)} recent ArXiv papers.")
        gr.DataFrame(
            all_papers,
            headers=["Title", "Abstract"],
            datatype=["markdown", "str"],
            max_height="1000",
            col_count=(2, "fixed"),
            column_widths=["30%", "70%"],
            wrap=True,
            interactive=True,
            render=True,
        )

    demo.launch(share=True, debug=True)

